{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b7b523ea",
   "metadata": {},
   "source": [
    "# Homework 4 - Hard coding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e1f852dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting ipynb\n",
      "  Downloading ipynb-0.5.1-py3-none-any.whl (6.9 kB)\n",
      "Installing collected packages: ipynb\n",
      "Successfully installed ipynb-0.5.1\n"
     ]
    }
   ],
   "source": [
    "!pip install ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f7581f0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting import-ipynb\n",
      "  Downloading import-ipynb-0.1.3.tar.gz (4.0 kB)\n",
      "Building wheels for collected packages: import-ipynb\n",
      "  Building wheel for import-ipynb (setup.py): started\n",
      "  Building wheel for import-ipynb (setup.py): finished with status 'done'\n",
      "  Created wheel for import-ipynb: filename=import_ipynb-0.1.3-py3-none-any.whl size=2975 sha256=400d82a1d217f9336fabe3748eae08741d9fb2cb8ee39380563442d94e900679\n",
      "  Stored in directory: c:\\users\\parsa33033\\appdata\\local\\pip\\cache\\wheels\\06\\7e\\ad\\1cb03e935234186825cefc7e2c8f3451b4f654b5bc72232a7b\n",
      "Successfully built import-ipynb\n",
      "Installing collected packages: import-ipynb\n",
      "Successfully installed import-ipynb-0.1.3\n"
     ]
    }
   ],
   "source": [
    "!pip install import-ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39b386b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install librosa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "2ba684e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import librosa\n",
    "import import_ipynb\n",
    "from AudioSignals import *\n",
    "import sys,os\n",
    "import csv\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "import scipy as sp\n",
    "import time\n",
    "import pickle as pkl\n",
    "import hashlib"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6ed9cff",
   "metadata": {},
   "source": [
    "## 1. Implementing your own Shazam"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "493e2088",
   "metadata": {},
   "source": [
    "### 1.1 Getting your data!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 532,
   "id": "a5acf144",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████| 1413/1413 [00:00<00:00, 1414113.95it/s]\n"
     ]
    }
   ],
   "source": [
    "data_dir = \"data\"\n",
    "N_TRACKS = 1413\n",
    "\n",
    "if not os.path.exists(data_dir):\n",
    "    os.mkdir(data_dir)\n",
    "\n",
    "def convert_audio(audio_name, audio_path, converted_extention='wav', destination_path=\"data\"):\n",
    "    destination_audio = os.path.join(destination_path, audio_name[:-3] + converted_extention) \n",
    "    if not Path(destination_audio).exists():\n",
    "        subprocess.check_output(f\"ffmpeg -i {audio_path} {destination_audio}\", shell=True)\n",
    "        return destination_audio\n",
    "    return None\n",
    "\n",
    "with open(\"dataset.csv\", \"w\", newline='') as dataset:\n",
    "    writer = csv.writer(dataset)\n",
    "    writer.writerow([\"title\", \"band\", \"album\", \"track\"])\n",
    "    for path, subdirs, files in tqdm(os.walk(\"mp3s-32k\")):\n",
    "        for name in files:\n",
    "            f = path.split('\\\\')           \n",
    "            if len(f) == 3:\n",
    "                mp3_path = os.path.join(path, name)\n",
    "                wav_path = convert_audio(name, mp3_path)\n",
    "                if wav_path:\n",
    "                    writer.writerow([name, f[1], f[2], wav_path])\n",
    "ds = pd.read_csv(\"dataset.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67cd4b7b",
   "metadata": {},
   "source": [
    "### 1.2 Fingerprint hashing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 517,
   "id": "52718ac5",
   "metadata": {},
   "outputs": [],
   "source": [
    "N_TRACKS = 1413\n",
    "HOP_SIZE = 512\n",
    "DURATION = 30 # TODO: to be tuned!\n",
    "THRESHOLD = 0 # TODO: to be tuned!\n",
    "data_folder = Path(\"data/\")\n",
    "tracks = data_folder.glob(\"*.wav\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 518,
   "id": "5efeba6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "hash_sig_num = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 519,
   "id": "6eea0f4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "m = 62352\n",
    "c1 = 20899\n",
    "c2 = 139\n",
    "def h1(time, freq):\n",
    "    return (c1 * time +  c2 * freq) % m\n",
    "def h2(time, freq):\n",
    "    return (time +  freq) % m\n",
    "def h(time, freq):\n",
    "    g = []\n",
    "    for i in range(hash_sig_num):\n",
    "        g.append(h1(time, freq) + i * h2(time, freq))\n",
    "    g = np.array(g, dtype=np.int64)\n",
    "    return g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 520,
   "id": "069b729c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_peaks(amplitude):\n",
    "    freq, time = amplitude.shape\n",
    "    peaks = []\n",
    "    for t in range(time):\n",
    "        s = sp.signal.find_peaks(amplitude[:,t])[0]\n",
    "        x = np.sum([i * s[i] for i in range(len(s))])\n",
    "        if x == 0: continue\n",
    "        peaks.append([t, x])\n",
    "    peaks = np.array(peaks)\n",
    "    return peaks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 522,
   "id": "2388bd26",
   "metadata": {},
   "outputs": [],
   "source": [
    "def minhash_signature(amplitude):\n",
    "    peaks = find_peaks(amplitude)\n",
    "    minhash_sig = []\n",
    "    minhash_sig = np.min(h(peaks[:,0], peaks[:,1]), axis=1)\n",
    "    return minhash_sig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 523,
   "id": "92db4c87",
   "metadata": {},
   "outputs": [],
   "source": [
    "buckets = dict()\n",
    "buckets_num = 200\n",
    "band_num = 10\n",
    "band_width = hash_sig_num // band_num\n",
    "band = 4 \n",
    "band_start = 4 * band_width\n",
    "band_end = 4 * band_width + band_width"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 524,
   "id": "6b986ce1",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_file_name = 'dataset.csv'\n",
    "bucket_file_name = 'song_buckets.pkl'\n",
    "minhash_sig_file_name = 'song_minhash_signature.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 531,
   "id": "bfd1e330",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 1412/1412 [25:12<00:00,  1.07s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created minhash signature of 1413 songs in 1512.55 seconds.\n"
     ]
    }
   ],
   "source": [
    "t1 = time.time()\n",
    "with open(dataset_file_name, 'rb') as dataset:\n",
    "    ds = pd.read_csv(dataset)\n",
    "    with open(minhash_sig_file_name, 'w', newline=\"\") as minhash_dataset:\n",
    "        minhash_writer = csv.writer(minhash_dataset)\n",
    "        minhash_writer.writerow(['index', 'title', 'artist', 'minhash_signature'])\n",
    "        with open(bucket_file_name, 'wb') as buckets_file:\n",
    "            for idx, row in tqdm(ds.iterrows(), total=ds.shape[0]):\n",
    "#                 if idx == 100:\n",
    "#                     break\n",
    "                audio = row.track\n",
    "                artist = row.band\n",
    "                title = row.title[:-4].split('-')[1]\n",
    "                track, sr, onset_env, peaks = load_audio_picks(audio, DURATION, HOP_SIZE)\n",
    "                fft = librosa.stft(track)\n",
    "                fft_amplitude = librosa.amplitude_to_db(np.abs(fft), ref=np.max)\n",
    "                minhash_sig = minhash_signature(fft_amplitude)\n",
    "                minhash_writer.writerow([idx, title, artist, minhash_sig])\n",
    "                minhash_band = minhash_sig[band_start: band_end]\n",
    "                bucket = (3 * hash(np.sum(minhash_band)) + 7) % buckets_num\n",
    "                if bucket in buckets.keys():\n",
    "                    buckets[bucket].append([idx, minhash_band])\n",
    "                else:\n",
    "                    buckets[bucket] = []\n",
    "                    buckets[bucket].append([idx, minhash_band])\n",
    "                    \n",
    "            pkl.dump(buckets, buckets_file)\n",
    "print(\"Created minhash signature of \" + str(N_TRACKS) + \" songs in\", \"{:.2f}\".format(time.time()-t1), \"seconds.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2b33df6",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(bucket_file_name, 'rb') as d:\n",
    "    buckets = pkl.load(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 533,
   "id": "54f32eec",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-533-a141aaa96d22>:11: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  b = np.array(buckets[bucket])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "track1.wav is ----> Dream_On by aerosmith , which was found in 0.02 seconds.\n",
      "track2.wav is ----> I_Want_To_Break_Free by queen , which was found in 0.02 seconds.\n",
      "track3.wav is ----> October by u2 , which was found in 0.02 seconds.\n",
      "track4.wav is ----> Ob by beatles , which was found in 0.02 seconds.\n",
      "track5.wav is ----> Karma_Police by radiohead , which was found in 0.02 seconds.\n",
      "track6.wav is ----> Heartbreaker by led_zeppelin , which was found in 0.02 seconds.\n",
      "track7.wav is ----> Go_Your_Own_Way by fleetwood_mac , which was found in 0.02 seconds.\n",
      "track8.wav is ----> American_Idiot by green_day , which was found in 0.02 seconds.\n",
      "track9.wav is ----> Somebody by depeche_mode , which was found in 0.02 seconds.\n",
      "track10.wav is ----> Black_Friday by steely_dan , which was found in 0.02 seconds.\n"
     ]
    }
   ],
   "source": [
    "for query_num in range(1,11):\n",
    "    track, sr, onset_env, peaks = load_audio_picks(\"query/track\"+str(query_num)+\".wav\", DURATION, HOP_SIZE)\n",
    "    fft = librosa.stft(track)\n",
    "    fft_amplitude = librosa.amplitude_to_db(np.abs(fft), ref=np.max)\n",
    "    minhash_sig = minhash_signature(fft_amplitude)\n",
    "    minhash_band = minhash_sig[band_start: band_end]\n",
    "    bucket = (3 * hash(np.sum(minhash_band)) + 7) % buckets_num\n",
    "    t1 = time.time()\n",
    "    with open(minhash_sig_file_name, 'r', newline=\"\") as minhash_dataset:\n",
    "        mhds = pd.read_csv(minhash_dataset)\n",
    "        b = np.array(buckets[bucket])\n",
    "        for idx, arr in zip(b[:,0], b[:,1]):\n",
    "            if str(minhash_band) == str(arr):\n",
    "                if str(mhds['minhash_signature'][idx]) == str(minhash_sig):\n",
    "                    print(\"track\"+str(query_num)+\".wav is ---->\",mhds['title'][idx], \"by\", mhds['artist'][idx],\", which was found in\", \"{:.2f}\".format(time.time()-t1), \"seconds.\")\n",
    "                    break \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1888bb06",
   "metadata": {},
   "source": [
    "## 2. Grouping songs together!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb8def99",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "265782b9",
   "metadata": {},
   "source": [
    "### 2.1 Getting your data!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f995c925",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ba0b0fb1",
   "metadata": {},
   "source": [
    "### 2.2 Choose your features (variables)!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0aded67",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0793e72c",
   "metadata": {},
   "source": [
    "### 2.3 Clustering!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddd58d13",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e527d0cc",
   "metadata": {},
   "source": [
    "### 2.4 Analysing your results!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b31e36de",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "029445bc",
   "metadata": {},
   "source": [
    "## Bonus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6e33c09",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "369b2cf1",
   "metadata": {},
   "source": [
    "## 3. Algorithmic questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 720,
   "id": "9497ab77",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sum_pairs(arr, n, s):\n",
    "    s_set, l = set(), set()\n",
    "    for i in range(n):\n",
    "        s_set.add(arr[i])\n",
    "        if s - arr[i] in s_set:\n",
    "            l.add((arr[i], s - arr[i]))\n",
    "    return l    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 721,
   "id": "d5ed1d56",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{(-3, 7), (2, 2), (3, 1), (6, -2)}"
      ]
     },
     "execution_count": 721,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A = [7, -2, 8, 2, 6, 4, -7, 2, 1, 3, -3]\n",
    "get_sum_pairs(A, len(A), 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7cd214c",
   "metadata": {},
   "source": [
    "Since there is one for loop which goes over all the elements of the array only once, the time complexity is of $O(n)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e92a58b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
