{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b7b523ea",
   "metadata": {},
   "source": [
    "# Homework 4 - Hard coding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e1f852dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting ipynb\n",
      "  Downloading ipynb-0.5.1-py3-none-any.whl (6.9 kB)\n",
      "Installing collected packages: ipynb\n",
      "Successfully installed ipynb-0.5.1\n"
     ]
    }
   ],
   "source": [
    "!pip install ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f7581f0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting import-ipynb\n",
      "  Downloading import-ipynb-0.1.3.tar.gz (4.0 kB)\n",
      "Building wheels for collected packages: import-ipynb\n",
      "  Building wheel for import-ipynb (setup.py): started\n",
      "  Building wheel for import-ipynb (setup.py): finished with status 'done'\n",
      "  Created wheel for import-ipynb: filename=import_ipynb-0.1.3-py3-none-any.whl size=2975 sha256=400d82a1d217f9336fabe3748eae08741d9fb2cb8ee39380563442d94e900679\n",
      "  Stored in directory: c:\\users\\parsa33033\\appdata\\local\\pip\\cache\\wheels\\06\\7e\\ad\\1cb03e935234186825cefc7e2c8f3451b4f654b5bc72232a7b\n",
      "Successfully built import-ipynb\n",
      "Installing collected packages: import-ipynb\n",
      "Successfully installed import-ipynb-0.1.3\n"
     ]
    }
   ],
   "source": [
    "!pip install import-ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39b386b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install librosa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "2ba684e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import librosa\n",
    "import import_ipynb\n",
    "from AudioSignals import *\n",
    "import sys,os\n",
    "import csv\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "import scipy as sp\n",
    "import time\n",
    "import pickle as pkl\n",
    "import hashlib"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6ed9cff",
   "metadata": {},
   "source": [
    "## 1. Implementing your own Shazam"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "493e2088",
   "metadata": {},
   "source": [
    "### 1.1 Getting your data!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1826b38",
   "metadata": {},
   "source": [
    "We have not used the all the utility functions provided in AudioSignals.ipynb as we thought it was more intuitive to collect wav format of all the songs in one folder called \"data\" and try to create a dataframe called \"dataset\" which contains all the information about the songs such as its title, band name, album name, and the path to the track which is in \"data\" folder.\n",
    "\n",
    "By this way we could sumulate a database using a csv file named \"dataset.csv\" and fetch the songs along with their information from this supposed database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 532,
   "id": "a5acf144",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████| 1413/1413 [00:00<00:00, 1414113.95it/s]\n"
     ]
    }
   ],
   "source": [
    "data_dir = \"data\"\n",
    "N_TRACKS = 1413\n",
    "\n",
    "if not os.path.exists(data_dir):\n",
    "    os.mkdir(data_dir)\n",
    "\n",
    "def convert_audio(audio_name, audio_path, converted_extention='wav', destination_path=\"data\"):\n",
    "    destination_audio = os.path.join(destination_path, audio_name[:-3] + converted_extention) \n",
    "    if not Path(destination_audio).exists():\n",
    "        subprocess.check_output(f\"ffmpeg -i {audio_path} {destination_audio}\", shell=True)\n",
    "        return destination_audio\n",
    "    return None\n",
    "\n",
    "with open(\"dataset.csv\", \"w\", newline='') as dataset:\n",
    "    writer = csv.writer(dataset)\n",
    "    writer.writerow([\"title\", \"band\", \"album\", \"track\"])\n",
    "    for path, subdirs, files in tqdm(os.walk(\"mp3s-32k\")):\n",
    "        for name in files:\n",
    "            f = path.split('\\\\')           \n",
    "            if len(f) == 3:\n",
    "                mp3_path = os.path.join(path, name)\n",
    "                wav_path = convert_audio(name, mp3_path)\n",
    "                if wav_path:\n",
    "                    writer.writerow([name, f[1], f[2], wav_path])\n",
    "ds = pd.read_csv(\"dataset.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d92ede8",
   "metadata": {},
   "source": [
    "As we can see from the code above we have done the following:\n",
    "1. open a csv file called dataset\n",
    "2. loop over each song in the mp3s-32k, which is the directory containing all the mp3 songs\n",
    "3. convert the mp3 songs to wav using convert_audio function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67cd4b7b",
   "metadata": {},
   "source": [
    "### 1.2 Fingerprint hashing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14134d50",
   "metadata": {},
   "source": [
    "Now we need to create a minhash signature of each song and assign a band of it to an appropriate bucket so that we can find our query tracks using these bands from minhash signatures provided in the buckets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 517,
   "id": "52718ac5",
   "metadata": {},
   "outputs": [],
   "source": [
    "N_TRACKS = 1413\n",
    "HOP_SIZE = 512\n",
    "DURATION = 30 # TODO: to be tuned!\n",
    "THRESHOLD = 0 # TODO: to be tuned!\n",
    "data_folder = Path(\"data/\")\n",
    "tracks = data_folder.glob(\"*.wav\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 518,
   "id": "5efeba6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "hash_sig_num = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6ce1366",
   "metadata": {},
   "source": [
    "Now we need to create our hash functions.\n",
    "\n",
    "As mentinoed in the paper by Kirsch, Adam, and Michael Mitzenmacher with the title of \"Less hashing, same performance: Building a better Bloom filter.\", if we can create two uniform hash functions, then we can use these to hash functions to create as much other hash functions as we need with the formula below:\n",
    "\n",
    "$g_{i}(x) = h_{1}(x) + ih_{2}(x) : 0<i \\leq hash\\_sig\\_num$\n",
    "\n",
    "The hash_sig_num is actually the number of minhash fucntions which will cause the minhash signature array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 519,
   "id": "6eea0f4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "m = 62352\n",
    "c1 = 20899\n",
    "c2 = 139\n",
    "def h1(time, freq):\n",
    "    return (c1 * time +  c2 * freq) % m\n",
    "def h2(time, freq):\n",
    "    return (time +  freq) % m\n",
    "def h(time, freq):\n",
    "    g = []\n",
    "    for i in range(hash_sig_num):\n",
    "        g.append(h1(time, freq) + i * h2(time, freq))\n",
    "    g = np.array(g, dtype=np.int64)\n",
    "    return g"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91071673",
   "metadata": {},
   "source": [
    "As it is evident from the code above we created two hash functions and used them in the formula mentioned. The hash functions are as below:\n",
    "\n",
    "$h_{1} = (c_{1}time + c_{2}freq) \\ Mod \\ m $\n",
    "\n",
    "$h_{2} = (time + freq) \\ Mod \\ m$\n",
    "\n",
    "where  $m,c_{1},c_{2}$ are constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 520,
   "id": "0096ca0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_peaks(amplitude):\n",
    "    freq, time = amplitude.shape\n",
    "    peaks = []\n",
    "    for t in range(time):\n",
    "        s = sp.signal.find_peaks(amplitude[:,t])[0]\n",
    "        x = np.sum([i * s[i] for i in range(len(s))])\n",
    "        if x == 0: continue\n",
    "        peaks.append([t, x])\n",
    "    peaks = np.array(peaks)\n",
    "    return peaks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf86a6fa",
   "metadata": {},
   "source": [
    "The find_peaks function receives the fourier amplitudes from the spectogram and finds the peaks within each time slot and appends them to a list as a list of time and frequency [t,f].\n",
    "\n",
    "$f = 1 \\times f_{1} + 2 \\times f_{2} + 3 \\times f_{3} + ... + n \\times f_{n}$ \n",
    "\n",
    "$result = [[t_{1}, f_{t_{1}}], [t_{2}, f_{t_{2}}], ..., [t_{k}, f_{t_{k}}]]$\n",
    "\n",
    "The minhash_signauter function hashes each of the elements from the list received from the find_peaks function and creates a minhash signature for each song"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 522,
   "id": "2388bd26",
   "metadata": {},
   "outputs": [],
   "source": [
    "def minhash_signature(amplitude):\n",
    "    peaks = find_peaks(amplitude)\n",
    "    minhash_sig = []\n",
    "    minhash_sig = np.min(h(peaks[:,0], peaks[:,1]), axis=1)\n",
    "    return minhash_sig"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e326bed9",
   "metadata": {},
   "source": [
    "#### Now we need to take the dataset.csv and create the minhash signature for each of the songs and create a csv file indicating the minhash signatures related to each song"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 523,
   "id": "92db4c87",
   "metadata": {},
   "outputs": [],
   "source": [
    "buckets = dict()\n",
    "buckets_num = 200\n",
    "band_num = 10\n",
    "band_width = hash_sig_num // band_num\n",
    "band = 4 \n",
    "band_start = 4 * band_width\n",
    "band_end = 4 * band_width + band_width"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 524,
   "id": "6b986ce1",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_file_name = 'dataset.csv'\n",
    "bucket_file_name = 'song_buckets.pkl'\n",
    "minhash_sig_file_name = 'song_minhash_signature.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 531,
   "id": "bfd1e330",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 1412/1412 [25:12<00:00,  1.07s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created minhash signature of 1413 songs in 1512.55 seconds.\n"
     ]
    }
   ],
   "source": [
    "t1 = time.time()\n",
    "with open(dataset_file_name, 'rb') as dataset:\n",
    "    ds = pd.read_csv(dataset)\n",
    "    with open(minhash_sig_file_name, 'w', newline=\"\") as minhash_dataset:\n",
    "        minhash_writer = csv.writer(minhash_dataset)\n",
    "        minhash_writer.writerow(['index', 'title', 'artist', 'minhash_signature'])\n",
    "        with open(bucket_file_name, 'wb') as buckets_file:\n",
    "            for idx, row in tqdm(ds.iterrows(), total=ds.shape[0]):\n",
    "#                 if idx == 100:\n",
    "#                     break\n",
    "                audio = row.track\n",
    "                artist = row.band\n",
    "                title = row.title[:-4].split('-')[1]\n",
    "                track, sr, onset_env, peaks = load_audio_picks(audio, DURATION, HOP_SIZE)\n",
    "                fft = librosa.stft(track)\n",
    "                fft_amplitude = librosa.amplitude_to_db(np.abs(fft), ref=np.max)\n",
    "                minhash_sig = minhash_signature(fft_amplitude)\n",
    "                minhash_writer.writerow([idx, title, artist, minhash_sig])\n",
    "                minhash_band = minhash_sig[band_start: band_end]\n",
    "                bucket = (3 * hash(np.sum(minhash_band)) + 7) % buckets_num\n",
    "                if bucket in buckets.keys():\n",
    "                    buckets[bucket].append([idx, minhash_band])\n",
    "                else:\n",
    "                    buckets[bucket] = []\n",
    "                    buckets[bucket].append([idx, minhash_band])\n",
    "                    \n",
    "            pkl.dump(buckets, buckets_file)\n",
    "print(\"Created minhash signature of \" + str(N_TRACKS) + \" songs in\", \"{:.2f}\".format(time.time()-t1), \"seconds.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2da5db6",
   "metadata": {},
   "source": [
    "As it is evident from the code above, we are doing the following:\n",
    "\n",
    "1. read the dataset.csv file in order to fetch the songs in a datafram named \"ds\"\n",
    "2. open a song_minhash_signature.csv file in order to write the minhash signature of each song in it.\n",
    "3. open a song_buckets.pkl file for the dictionary affiliated with the buckets of the bands within each minhash signature\n",
    "4. loop through each of the items of the dataset \"ds\"\n",
    "5. for each song fetch the song and create a minhash signature for it\n",
    "6. take a band from the signature and assign it as a value to the bucket with the key of the hash of the band's sum.\n",
    "7. write the minhash signature along with title of the song and name of the artist in the  dataset.csv file in order to fetch the songs in a datafram named \"ds\" song_minhash_signature.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2b33df6",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(bucket_file_name, 'rb') as d:\n",
    "    buckets = pkl.load(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 533,
   "id": "54f32eec",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-533-a141aaa96d22>:11: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  b = np.array(buckets[bucket])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "track1.wav is ----> Dream_On by aerosmith , which was found in 0.02 seconds.\n",
      "track2.wav is ----> I_Want_To_Break_Free by queen , which was found in 0.02 seconds.\n",
      "track3.wav is ----> October by u2 , which was found in 0.02 seconds.\n",
      "track4.wav is ----> Ob by beatles , which was found in 0.02 seconds.\n",
      "track5.wav is ----> Karma_Police by radiohead , which was found in 0.02 seconds.\n",
      "track6.wav is ----> Heartbreaker by led_zeppelin , which was found in 0.02 seconds.\n",
      "track7.wav is ----> Go_Your_Own_Way by fleetwood_mac , which was found in 0.02 seconds.\n",
      "track8.wav is ----> American_Idiot by green_day , which was found in 0.02 seconds.\n",
      "track9.wav is ----> Somebody by depeche_mode , which was found in 0.02 seconds.\n",
      "track10.wav is ----> Black_Friday by steely_dan , which was found in 0.02 seconds.\n"
     ]
    }
   ],
   "source": [
    "for query_num in range(1,11):\n",
    "    track, sr, onset_env, peaks = load_audio_picks(\"query/track\"+str(query_num)+\".wav\", DURATION, HOP_SIZE)\n",
    "    fft = librosa.stft(track)\n",
    "    fft_amplitude = librosa.amplitude_to_db(np.abs(fft), ref=np.max)\n",
    "    minhash_sig = minhash_signature(fft_amplitude)\n",
    "    minhash_band = minhash_sig[band_start: band_end]\n",
    "    bucket = (3 * hash(np.sum(minhash_band)) + 7) % buckets_num\n",
    "    t1 = time.time()\n",
    "    with open(minhash_sig_file_name, 'r', newline=\"\") as minhash_dataset:\n",
    "        mhds = pd.read_csv(minhash_dataset)\n",
    "        b = np.array(buckets[bucket])\n",
    "        for idx, arr in zip(b[:,0], b[:,1]):\n",
    "            if str(minhash_band) == str(arr):\n",
    "                if str(mhds['minhash_signature'][idx]) == str(minhash_sig):\n",
    "                    print(\"track\"+str(query_num)+\".wav is ---->\",mhds['title'][idx], \"by\", mhds['artist'][idx],\", which was found in\", \"{:.2f}\".format(time.time()-t1), \"seconds.\")\n",
    "                    break \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea2874b7",
   "metadata": {},
   "source": [
    "1. For the query, we fetch the minhash_signature of the unknown track and take the specific band out. hash the sum of the picked band from the signature and take the hash of the sum.\n",
    "2. Search the bucket's keys for this hashed value and fetch all the minhash signature bands within that key (each key contains a list of minhash signature bands). \n",
    "3. check the picked bucket for all the minhash signatures and find the correct match and give back the title of the song and the name of its artist\n",
    "\n",
    "\n",
    "As it is shown the tracks' info are as below:\n",
    "\n",
    "\n",
    "| track #  | artist | song title |\n",
    "| --- | --- | --- |\n",
    "| track1.wav | aerosmith | dream on |\n",
    "| track2.wav | queen  | I Want To Break Free  |\n",
    "| track3.wav | u2  | October  |\n",
    "| track4.wav | beatles  | Ob  |\n",
    "| track5.wav | radiohead  | Karma Police  |\n",
    "| track6.wav | led zeppelin  | Heartbreaker  |\n",
    "| track7.wav | fleetwood mac  | Go Your Own Way  |\n",
    "| track8.wav | green day  | American Idiot  |\n",
    "| track9.wav | depeche mode  | Somebody  |\n",
    "| track10.wav | steely dan  | Black Friday  |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1888bb06",
   "metadata": {},
   "source": [
    "## 2. Grouping songs together!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb8def99",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "265782b9",
   "metadata": {},
   "source": [
    "### 2.1 Getting your data!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f995c925",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ba0b0fb1",
   "metadata": {},
   "source": [
    "### 2.2 Choose your features (variables)!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0aded67",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0793e72c",
   "metadata": {},
   "source": [
    "### 2.3 Clustering!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddd58d13",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e527d0cc",
   "metadata": {},
   "source": [
    "### 2.4 Analysing your results!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b31e36de",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "029445bc",
   "metadata": {},
   "source": [
    "## Bonus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6e33c09",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "369b2cf1",
   "metadata": {},
   "source": [
    "## 3. Algorithmic questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 720,
   "id": "9497ab77",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sum_pairs(arr, n, s):\n",
    "    s_set, l = set(), set()\n",
    "    for i in range(n):\n",
    "        s_set.add(arr[i])\n",
    "        if s - arr[i] in s_set:\n",
    "            l.add((arr[i], s - arr[i]))\n",
    "    return l    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 721,
   "id": "d5ed1d56",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{(-3, 7), (2, 2), (3, 1), (6, -2)}"
      ]
     },
     "execution_count": 721,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A = [7, -2, 8, 2, 6, 4, -7, 2, 1, 3, -3]\n",
    "get_sum_pairs(A, len(A), 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7cd214c",
   "metadata": {},
   "source": [
    "Since there is one for loop which goes over all the elements of the array only once, the time complexity is of $O(n)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e92a58b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
